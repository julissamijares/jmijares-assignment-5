{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean', batch_size=500):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Set feature weights based on correlation\n",
    "        self.weights = {\n",
    "            'Age': 1.0,  # Highest positive correlation\n",
    "            'NumOfProducts': 0.5,  # Negative correlation, somewhat important\n",
    "            'Geography_Germany': 0.75,  # Positive correlation\n",
    "            'Gender_Male': 0.5,  # Negative correlation, somewhat important\n",
    "            'Balance': 0.6,  # Positive correlation\n",
    "            'HasCrCard': 0.2,  # Not important\n",
    "            'IsActiveMember': 0.1,  # Not important\n",
    "            'Geography_France': 0.2,  # Not important\n",
    "            'Geography_Spain': 0.2,  # Not important\n",
    "            'Gender_Female': 0.5,  # Not important\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = []\n",
    "\n",
    "        # Process in batches\n",
    "        for i in range(0, n_samples, self.batch_size):\n",
    "            X_batch = X[i:i + self.batch_size]\n",
    "            distances = self.compute_distances(X_batch)\n",
    "            k_nearest_indices = np.argsort(distances, axis=1)[:, :self.k]\n",
    "            k_nearest_labels = self.y_train[k_nearest_indices]\n",
    "\n",
    "            # Find the most common label in the k nearest neighbors for each test point\n",
    "            batch_predictions = [max(set(labels), key=labels.tolist().count) for labels in k_nearest_labels]\n",
    "            predictions.extend(batch_predictions)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def compute_distances(self, X_batch):\n",
    "        # Calculate weighted distances\n",
    "        weights = np.array([self.weights.get(col, 1.0) for col in range(self.X_train.shape[1])])\n",
    "        \n",
    "        if self.distance_metric == 'euclidean':\n",
    "            # Broadcasting for efficient Euclidean distance computation\n",
    "            weighted_X_train = self.X_train * weights\n",
    "            weighted_X_batch = X_batch * weights\n",
    "            dists = np.sqrt(np.sum((weighted_X_batch[:, np.newaxis] - weighted_X_train) ** 2, axis=2))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            weighted_X_train = self.X_train * weights\n",
    "            weighted_X_batch = X_batch * weights\n",
    "            dists = np.sum(np.abs(weighted_X_batch[:, np.newaxis] - weighted_X_train), axis=2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid distance metric\")\n",
    "        \n",
    "        return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_path, test_path):\n",
    "    # Load training data\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    # Handle missing values\n",
    "    numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "    categorical_features = ['Geography', 'Gender']\n",
    "\n",
    "    # Fill missing values for numerical features with the median for both train and test\n",
    "    for feature in numerical_features:\n",
    "        train_data[feature].fillna(train_data[feature].median(), inplace=True)\n",
    "        test_data[feature].fillna(train_data[feature].median(), inplace=True)\n",
    "\n",
    "    # Fill missing values for categorical features with the mode for both train and test\n",
    "    for feature in categorical_features:\n",
    "        train_data[feature].fillna(train_data[feature].mode().iloc[0], inplace=True)\n",
    "        test_data[feature].fillna(train_data[feature].mode().iloc[0], inplace=True)\n",
    "\n",
    "    # One-hot encode categorical features for both train and test\n",
    "    train_data = pd.get_dummies(train_data, columns=categorical_features, drop_first=True)\n",
    "    test_data = pd.get_dummies(test_data, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    # Align the test data to have the same columns as the train data\n",
    "    test_data = test_data.reindex(columns=train_data.columns.drop('Exited'), fill_value=0)\n",
    "\n",
    "    # Drop unnecessary columns for training\n",
    "    columns_to_drop = ['CustomerId', 'Surname', 'CreditScore', 'Tenure', 'EstimatedSalary', 'Exited', 'id']\n",
    "    X = train_data.drop(columns=columns_to_drop, axis=1)\n",
    "    test_data = test_data.drop(columns=['CustomerId', 'Surname', 'CreditScore', 'Tenure', 'EstimatedSalary', 'id'], axis=1)\n",
    "    y = train_data['Exited']\n",
    "\n",
    "    # Print the list of remaining columns\n",
    "    print(\"Remaining columns after dropping from training data:\", X.columns.tolist())\n",
    "\n",
    "    # Change the data type to float64\n",
    "    X = X.astype(np.float64)\n",
    "    test_data = test_data.astype(np.float64)\n",
    "\n",
    "    # Min-Max Scaling for numerical features\n",
    "    min_values = X.min()\n",
    "    max_values = X.max()\n",
    "\n",
    "    X = (X - min_values) / (max_values - min_values)  # Scale numerical features\n",
    "    test_data = (test_data - min_values) / (max_values - min_values)  # Scale test data\n",
    "\n",
    "    return X.values, y.values, test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation function\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    # TODO: Implement cross-validation\n",
    "    # Compute ROC AUC scores\n",
    "    n = len(X)\n",
    "    fold_size = n // n_splits\n",
    "    scores = []\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        X_train = np.concatenate([X[:i * fold_size], X[(i + 1) * fold_size:]], axis=0)\n",
    "        y_train = np.concatenate([y[:i * fold_size], y[(i + 1) * fold_size:]], axis=0)\n",
    "        X_val = X[i * fold_size: (i + 1) * fold_size]\n",
    "        y_val = y[i * fold_size: (i + 1) * fold_size]\n",
    "\n",
    "        knn.fit(X_train, y_train)\n",
    "        predictions = knn.predict(X_val)\n",
    "\n",
    "        accuracy = np.mean(predictions == y_val)\n",
    "        scores.append(accuracy)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns after dropping from training data: ['Age', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Geography_Germany', 'Geography_Spain', 'Gender_Male']\n",
      "Cross-validation scores: [0.8753333333333333, 0.8873333333333333, 0.875, 0.885, 0.8813333333333333]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('data/train.csv', 'data/test.csv')\n",
    "\n",
    "# Create and evaluate model\n",
    "knn = KNN(k=5, distance_metric='euclidean')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validate(X, y, knn)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 11, Distance Metric: manhattan, Cross-validation score: 0.8845333333333334\n"
     ]
    }
   ],
   "source": [
    "# TODO: hyperparamters tuning\n",
    "k_values = list(range(2, 16))  # Explore a wider range of k values\n",
    "distance_metrics = ['euclidean', 'manhattan']\n",
    "best_distance = 'euclidean'\n",
    "best_k = 3\n",
    "best_score = 0\n",
    "\n",
    "for metric in distance_metrics:\n",
    "    for k in k_values:\n",
    "        knn = KNN(k=k, distance_metric=metric)\n",
    "        cv_scores = cross_validate(X, y, knn)\n",
    "        avg_score = np.mean(cv_scores)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_k = k\n",
    "            best_distance = metric\n",
    "\n",
    "print(f\"Best k: {best_k}, Distance Metric: {best_distance}, Cross-validation score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train on full dataset with optimal hyperparameters and make predictions on test set\n",
    "knn = KNN(k=best_k, distance_metric=best_distance)\n",
    "knn.fit(X, y)\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "\n",
    "# Save test predictions\n",
    "pd.DataFrame({'id': pd.read_csv('data/test.csv')['id'], 'Exited': test_predictions}).to_csv('data/submissions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
